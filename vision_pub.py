#!/usr/bin/env python3
"""
æ–‡ä»¶: vision_pub.py (V3.2 å†…éƒ¨èåˆç‰ˆ)
åŠŸèƒ½: å¤šæ‘„è§†è§‰æ„ŸçŸ¥ã€å†…éƒ¨èåˆã€æ‹©ä¼˜å‘é€
æ›´æ–°æ—¥å¿—:
  1. æ¶æ„å‡çº§: é‡‡ç”¨ "æ”¶é›† -> æ’åº -> å»é‡ -> å‘é€" çš„ 20Hz å‘¨æœŸæ€§é€»è¾‘ã€‚
  2. èåˆç­–ç•¥: ç›¸åŒè§’åº¦çš„ç‰©ä½“ï¼Œä¿ç•™æ£€æµ‹æ¡†é¢ç§¯(Area)æœ€å¤§çš„é‚£ä¸ªã€‚
  3. æµ‹è·ç®—æ³•: çº¯å®½åº¦æ³• (Width-Based)ï¼Œç§»é™¤äº†å‡ ä½•æ³•ã€‚
  4. è¾¹ç¼˜æ£€æµ‹: è¯†åˆ«å¹¶æ ‡è®°è¢«æˆªæ–­çš„ç‰©ä½“ã€‚
"""

import os
import sys
import cv2
import numpy as np
import time
import math
import json
import zmq
import queue
import threading
from collections import defaultdict, deque
from ultralytics import YOLO

# ================= é…ç½®åŒºåŸŸ =================
MODEL_PATH = "/home/nvidia/Downloads/Ros/ballCar2/weights/weights/best.engine"
CAMERA_INDICES = [0, 2, 4, 6]
ZMQ_PORT = 5555
FUSION_RATE_HZ = 20        # èåˆä¸å‘é€é¢‘ç‡ (20Hz = 50ms)
STATE_LOCK_DURATION = 0.5  # çŠ¶æ€åˆ‡æ¢é”å®šæ—¶é—´

# === ç‰©ç†å‚æ•° ===
# çœŸå®å®½åº¦ (å•ä½: ç±³)
CLASS_REAL_WIDTHS = {
    "car": 0.31,
    "basketball": 0.23,
    "flag": 0.13
}

# ç›¸æœºå†…å‚ (æ ‡å®šåˆ†è¾¨ç‡: 640x480)
CAM_INTRINSICS = {
    'fx': 498.0,
    'fy': 498.0,
    'cx': 331.2797,
    'cy': 156.1371,
    'calib_width': 640,
    'calib_height': 480
}

# Class ID æ˜ å°„
CLS_MAP = {
    0: "OFF", 1: "BLUE", 2: "RED", 3: "GREEN", 
    4: "PURPLE", 5: "GRAY", 6: "BALL", 7: "FLAG"
}

# ç›¸æœºæœå‘ (æœºèº«åæ ‡ç³»)
CAM_MOUNT_YAW_DEG = {0: 180.0, 2: -90.0, 4: 0.0, 6: 90.0}

# ================= è¾…åŠ©å‡½æ•° =================
def calculate_azimuth_planar(x_pixel, fx, cx):
    """è®¡ç®—åƒå¹³é¢å†…çš„åèˆªè§’"""
    pixel_offset = x_pixel - cx
    angle_rad = math.atan(pixel_offset / fx)
    return math.degrees(angle_rad)

def wrap_deg_360(a):
    """å½’ä¸€åŒ–åˆ° 0-360 åº¦"""
    return (a + 360.0) % 360.0

def get_angle_diff(a1, a2):
    """è®¡ç®—ä¸¤ä¸ªè§’åº¦çš„æœ€å°å·®å€¼ (è€ƒè™‘0/360å¾ªç¯)"""
    diff = abs(a1 - a2)
    return min(diff, 360.0 - diff)

# ================= ä¸»ç±»å®šä¹‰ =================
class VisionPublisher:
    def __init__(self, model_path, camera_indices, zmq_port=5555):
        self.model_path = model_path
        self.camera_indices = camera_indices

        # æ ¸å¿ƒå‚æ•°
        self.fx = CAM_INTRINSICS['fx']
        self.cx = CAM_INTRINSICS['cx']
        self.calib_width = CAM_INTRINSICS['calib_width']
        self.calib_height = CAM_INTRINSICS['calib_height']

        # å®é™…è¿è¡Œåˆ†è¾¨ç‡
        self.actual_width = 640
        self.actual_height = 480

        # é€šä¿¡ä¸é˜Ÿåˆ—
        self.queue = queue.Queue(maxsize=200) # ç¨å¾®åŠ å¤§é˜Ÿåˆ—ï¼Œé˜²æ­¢å¤šæ‘„æ•°æ®ç¬é—´å †ç§¯
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.PUB)
        self.socket.bind(f"tcp://*:{zmq_port}")
        
        # çŠ¶æ€æ»¤æ³¢ç¼“å­˜
        self.history = defaultdict(lambda: deque(maxlen=30))
        self.state_memory = defaultdict(lambda: {'state': 0, 'pattern': 'OFF', 'last_switch_time': 0.0})

        print(f"âœ… è§†è§‰èåˆç³»ç»Ÿ V3.2 å¯åŠ¨ | é¢‘ç‡: {FUSION_RATE_HZ}Hz")

    def initialize_camera(self, cam_idx):
        try:
            cap = cv2.VideoCapture(cam_idx, cv2.CAP_V4L2)
            if not cap.isOpened(): cap = cv2.VideoCapture(cam_idx)
            if cap.isOpened():
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))

                # éªŒè¯å®é™…åˆ†è¾¨ç‡å¹¶è°ƒæ•´ fx
                actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                if actual_w != self.calib_width:
                    scale = actual_w / self.calib_width
                    self.fx = CAM_INTRINSICS['fx'] * scale
                    self.cx = CAM_INTRINSICS['cx'] * scale
                    self.actual_width = actual_w
                    self.actual_height = actual_h
                    print(f"âš ï¸ Cam {cam_idx}: åˆ†è¾¨ç‡ {actual_w}x{actual_h}, fx å·²ç¼©æ”¾è‡³ {self.fx:.2f}")

                return cap
        except Exception: pass
        return None

    def get_stable_state(self, cam_idx, track_id, current_class_id):
        """é¢œè‰²çŠ¶æ€é˜²æŠ–åŠ¨é€»è¾‘"""
        if current_class_id >= 6: return current_class_id, "SOLID" # çƒå’Œæ——å­ä¸æ»¤æ³¢

        key = (cam_idx, track_id)
        mem = self.state_memory[key]
        now = time.time()

        self.history[key].append(current_class_id)
        buffer = list(self.history[key])

        # ç®€å•çš„æŠ•ç¥¨æœºåˆ¶
        from collections import Counter
        colored_frames = [c for c in buffer if c in [1, 2, 3, 4, 5]]
        if not colored_frames: return 0, "OFF"
        
        dominant_color, count = Counter(colored_frames).most_common(1)[0]
        color_ratio = count / len(buffer)
        
        # æ¨¡å¼åˆ¤å®š (é—ªçƒ vs å¸¸äº®)
        target_pattern = 'SOLID' if color_ratio > 0.90 else 'FLASH'
        if mem['pattern'] == 'SOLID' and color_ratio < 0.80: target_pattern = 'FLASH'

        # çŠ¶æ€é”å®š (é˜²æ­¢é¢œè‰²å¹¶åœ¨è·³å˜)
        if (target_pattern != mem['pattern'] or dominant_color != mem['state']):
            if now - mem['last_switch_time'] > STATE_LOCK_DURATION:
                mem['state'] = dominant_color
                mem['pattern'] = target_pattern
                mem['last_switch_time'] = now
        
        return mem['state'], mem['pattern']

    def camera_worker(self, cam_idx):
        """
        ç›¸æœºå·¥ä½œçº¿ç¨‹ï¼šåªè´Ÿè´£æ£€æµ‹å’Œè®¡ç®—åŸºç¡€æ•°æ®ï¼Œä¸è´Ÿè´£èåˆã€‚
        """
        try:
            model = YOLO(self.model_path)
            self.queue.put({'type': 'log', 'message': f'âœ… Cam {cam_idx}: Ready'})
        except Exception as e:
            self.queue.put({'type': 'log', 'message': f'âŒ Cam {cam_idx}: {e}'})
            return

        cap = self.initialize_camera(cam_idx)
        if cap is None: return

        try:
            while True:
                ret, frame = cap.read()
                if not ret: 
                    time.sleep(0.1); continue

                results = model.track(frame, conf=0.45, iou=0.6, imgsz=(480, 640), persist=True, verbose=False)

                if results[0].boxes is not None:
                    for box in results[0].boxes:
                        raw_cls = int(box.cls.item())
                        tid = int(box.id.item()) if box.id is not None else -1
                        if tid < 0: continue

                        # 1. è·å–æ£€æµ‹æ¡†æ•°æ®
                        xyxy = box.xyxy[0].cpu().numpy()
                        x1, y1, x2, y2 = xyxy
                        box_w = x2 - x1
                        box_h = y2 - y1
                        x_center = (x1 + x2) / 2
                        
                        # è®¡ç®—é¢ç§¯
                        area = box_w * box_h

                        # 2. æˆªæ–­æ£€æµ‹ (Truncated)
                        # å¦‚æœæ¡†çš„è¾¹ç¼˜è´´è¿‘å›¾åƒè¾¹ç•Œ (å‡è®¾ 640x480)ï¼Œè¯´æ˜ç‰©ä½“å¯èƒ½ä¸å®Œæ•´
                        is_truncated = (x1 < 2 or y1 < 2 or x2 > 638 or y2 > 478)

                        # 3. çŠ¶æ€æ»¤æ³¢
                        stable_cls, pattern = self.get_stable_state(cam_idx, tid, raw_cls)
                        if stable_cls == 0: continue

                        # 4. çº¯å®½åº¦æµ‹è·
                        if stable_cls == 6: real_w = CLASS_REAL_WIDTHS["basketball"]
                        elif stable_cls == 7: real_w = CLASS_REAL_WIDTHS["flag"]
                        else: real_w = CLASS_REAL_WIDTHS["car"]

                        # ç¡®ä¿ box_w æœ‰æ•ˆ
                        if box_w <= 0:
                            continue

                        dist = (real_w * self.fx) / box_w

                        # 5. è§’åº¦è§£ç®—
                        azimuth = calculate_azimuth_planar(x_center, self.fx, self.cx)
                        bearing_body = wrap_deg_360(CAM_MOUNT_YAW_DEG.get(cam_idx, 0.0) + azimuth)

                        # 6. æ‰“åŒ…å…¥é˜Ÿ (åŒ…å« area å’Œ truncated ä¾›ä¸»çº¿ç¨‹èåˆä½¿ç”¨)
                        if dist < 8.0:
                            data = {
                                'type': 'detection',
                                'cam_idx': cam_idx,
                                'class_id': stable_cls,
                                'pattern': pattern,
                                'distance': round(dist, 2),
                                'bearing_body': round(bearing_body, 2),
                                'area': int(area),          # èåˆæƒé‡æ ¸å¿ƒ
                                'truncated': is_truncated   # é™æƒæ ‡å¿—
                            }
                            # éé˜»å¡å…¥é˜Ÿï¼Œæ»¡äº†å°±æ‰”æ‰æ—§çš„
                            try: self.queue.put_nowait(data)
                            except queue.Full: pass
                            
        except Exception as e:
            print(f"Cam {cam_idx} Error: {e}")
        finally:
            cap.release()

    def run(self):
        """
        ä¸»å¾ªç¯ï¼šå‘¨æœŸæ€§èåˆ (Batch Processing)
        """
        print(f'ğŸš€ è§†è§‰èåˆå¾ªç¯å¯åŠ¨... (å‘¨æœŸ: {1.0/FUSION_RATE_HZ*1000:.1f}ms)')
        
        # å¯åŠ¨çº¿ç¨‹
        for idx in self.camera_indices:
            threading.Thread(target=self.camera_worker, args=(idx,), daemon=True).start()

        fusion_interval = 1.0 / FUSION_RATE_HZ
        
        try:
            while True:
                cycle_start = time.time()
                
                # --- 1. æ”¶é›†é˜¶æ®µ (Drain Queue) ---
                # æŠŠå½“å‰ç¬é—´é˜Ÿåˆ—é‡Œæ‰€æœ‰ç›¸æœºçš„æ•°æ®å…¨å–å‡ºæ¥
                batch_detections = []
                while True:
                    try:
                        item = self.queue.get_nowait()
                        if item['type'] == 'log':
                            print(item['message'])
                        elif item['type'] == 'detection':
                            batch_detections.append(item)
                        self.queue.task_done()
                    except queue.Empty:
                        break # é˜Ÿåˆ—ç©ºäº†ï¼Œæ”¶é›†å®Œæ¯•
                
                # --- 2. èåˆé˜¶æ®µ (Fusion) ---
                final_objects = []

                if batch_detections:
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºæ”¶é›†åˆ°çš„åŸå§‹æ•°æ®
                    if len(batch_detections) > 1:
                        raw_info = [f"Cam{d['cam_idx']}:{CLS_MAP.get(d['class_id'],'?')}@{d['bearing_body']:.0f}Â°"
                                   for d in batch_detections]
                        print(f"  ğŸ” æ”¶é›†åˆ° {len(batch_detections)} ä¸ªæ£€æµ‹: {raw_info}")

                    # A. æ’åºï¼šé¢ç§¯å¤§çš„æ’å‰é¢ (ç›¸ä¿¡çœ‹å¾—æœ€æ¸…æ¥šçš„)
                    #    è¢«æˆªæ–­çš„ç‰©ä½“é™æƒå¤„ç†
                    def get_effective_area(obj):
                        area = obj['area']
                        if obj['truncated']:
                            area *= 0.5  # æˆªæ–­ç‰©ä½“æƒé‡å‡åŠ
                        return area

                    batch_detections.sort(key=get_effective_area, reverse=True)

                    # B. å»é‡ (Suppression) - ä»…åŸºäºè§’åº¦
                    for new_obj in batch_detections:
                        is_duplicate = False
                        duplicate_with = None
                        angle_diff = 0.0

                        for existing in final_objects:
                            # åˆ¤æ–­æ˜¯å¦ä¸ºåŒä¸€ç‰©ä½“ï¼š
                            # 1. ç±»åˆ«å¿…é¡»ç›¸åŒ
                            if new_obj['class_id'] != existing['class_id']:
                                continue

                            # 2. è§’åº¦æ¥è¿‘ (ç›¸å·® < 100 åº¦) - åªçœ‹è§’åº¦
                            angle_diff = get_angle_diff(new_obj['bearing_body'], existing['bearing_body'])

                            if angle_diff < 100:
                                is_duplicate = True
                                duplicate_with = existing
                                break

                        if is_duplicate:
                            # è°ƒè¯•ï¼šæ˜¾ç¤ºå»é‡ä¿¡æ¯
                            print(f"    âŒ å»é‡: Cam{new_obj['cam_idx']} çš„ {CLS_MAP.get(new_obj['class_id'],'?')} "
                                  f"ä¸ Cam{duplicate_with['cam_idx']} é‡å¤ (è§’åº¦å·®={angle_diff:.1f}Â°)")
                        else:
                            final_objects.append(new_obj)

                # --- 3. å‘é€é˜¶æ®µ (Publish) ---
                # [Refactor] Changed to batch sending
                # æ„é€ åŒ…å«æ‰€æœ‰ç‰©ä½“çš„å¤§å­—å…¸ (Payload)
                topic = "perception"
                timestamp = time.time()

                # æ„å»ºç‰©ä½“åˆ—è¡¨ï¼ŒåªåŒ…å«ä¸‹æ¸¸éœ€è¦çš„å­—æ®µ
                objects_list = []
                for obj in final_objects:
                    obj_data = {
                        'cam_idx': obj['cam_idx'],
                        'class_id': obj['class_id'],
                        'pattern': obj['pattern'],
                        'distance': obj['distance'],
                        'bearing_body': obj['bearing_body']
                    }
                    objects_list.append(obj_data)

                # æ„é€ å®Œæ•´çš„ Payload
                payload = {
                    'timestamp': timestamp,
                    'count': len(final_objects),
                    'objects': objects_list
                }

                # [Refactor] åªå‘é€ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯éå†å‘é€
                self.socket.send_string(f"{topic} {json.dumps(payload)}")

                # æ‰“å°å‘é€çš„ç‰©ä½“ä¿¡æ¯
                if final_objects:
                    print(f"ğŸ“¦ [Batch Send] æ—¶é—´æˆ³={timestamp:.3f} | ç‰©ä½“æ•°={len(final_objects)}")
                    for obj in final_objects:
                        cls_name = CLS_MAP.get(obj['class_id'], "UNK")
                        print(f"  ğŸ‘ï¸ [{cls_name}-Cam{obj['cam_idx']}] {obj['pattern']:<5} | D={obj['distance']:.2f}m | Ang={obj['bearing_body']:.1f}Â°")

                # --- 4. æ§é¢‘ä¼‘çœ  ---
                elapsed = time.time() - cycle_start
                sleep_time = fusion_interval - elapsed
                if sleep_time > 0:
                    time.sleep(sleep_time)

        except KeyboardInterrupt:
            print("ğŸ›‘ åœæ­¢ä¸­...")
        finally:
            self.socket.close()
            self.context.term()

if __name__ == "__main__":
    pub = VisionPublisher(MODEL_PATH, CAMERA_INDICES, ZMQ_PORT)
    pub.run()
# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import io
import os
import sys
import math
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple
from collections import defaultdict, deque

import cv2
import numpy as np
import torch

# ç¡®ä¿ä½¿ç”¨æœ¬åœ°ä¿®æ”¹çš„ultralyticsç‰ˆæœ¬
current_dir = os.path.dirname(os.path.abspath(__file__))
ultralytics_root = os.path.dirname(os.path.dirname(current_dir))
if ultralytics_root not in sys.path:
    sys.path.insert(0, ultralytics_root)

from ultralytics import YOLO
from ultralytics.utils import LOGGER
from ultralytics.utils.checks import check_requirements

# ã€æ–°å¢ã€‘å¯¼å…¥ 3D è¾…åŠ©å·¥å…·
try:
    from bbox3d_utils import BBox3DEstimator
except ImportError:
    LOGGER.error("âŒ æ— æ³•å¯¼å…¥ bbox3d_utils.pyã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•ä¸‹ã€‚")
    sys.exit(1)

torch.classes.__path__ = []  # Torch module __path__._path issue fix


class Inference:
    """
    Streamlit Interface for YOLO object detection, optimized for Multi-Camera.
    Includes Monocular Distance Estimation and Pseudo 3D visualization.
    """

    def __init__(self, **kwargs: Any) -> None:
        """
        Initialize the Inference class.
        """
        check_requirements("streamlit>=1.29.0")
        import streamlit as st

        self.st = st
        # å¼ºåˆ¶é»˜è®¤ä¸ºå¤šæ‘„åƒå¤´æ¨¡å¼
        self.source = "multi-webcam" 
        self.enable_trk = False
        self.conf = 0.25
        self.iou = 0.45
        self.selected_ind: List[int] = []
        self.model = None

        # Multi-camera support attributes
        self.num_cameras = 4
        self.camera_indices = [0, 2, 4, 6]
        
        # ç›¸æœºå®‰è£…æœå‘åç§» (æœºä½“åæ ‡ç³»)
        self.camera_yaw_offsets = {
            0: 180.0,  # å
            2: -90.0,  # å³
            4: 0.0,    # å‰
            6: 90.0    # å·¦
        }

        self.frame_width = 640
        self.frame_height = 480 # ç»Ÿä¸€ä½¿ç”¨ 640x480
        self.camera_caps: List[cv2.VideoCapture] = []
        self.camera_threads: List[threading.Thread] = []
        self.camera_frames: Dict[int, np.ndarray] = {}
        self.camera_running = False
        self.camera_containers: Dict[int, Tuple[Any, Any]] = {}

        # ç©ºé—´æµ‹é‡å‚æ•° (Triangulation / Monocular Estimation)
        # ç›¸æœºå†…å‚ (åŸºäº 640x480 çš„æ ‡å®šå€¼)
        self.camera_intrinsics = {
            'fx': 498.0,
            'fy': 498.0,
            'cx': 331.2797,
            'cy': 240.0 # å‡è®¾ 640x480 å±…ä¸­
        }

        # ä¸åŒç±»åˆ«çš„çœŸå®å®½åº¦ (ç±³)
        self.class_real_widths = {
            6: 0.23,  # å‡è®¾ Class 6 æ˜¯ç‰¹å®šç‰©ä½“ï¼Œå®½åº¦ 23cm
            **{i: 0.31 for i in range(6)}
        }
        self.default_real_width = 0.31

        # ã€æ–°å¢ã€‘3D è¾¹ç•Œæ¡†ä¼°ç®—å™¨ (ç”¨äºä¼ª 3D ç»˜å›¾)
        self.bbox3d_estimator = BBox3DEstimator()
        
        # ã€æ–°å¢ã€‘æ·±åº¦æ˜ å°„èŒƒå›´ï¼ˆç”¨äºå°†ç±³åˆ¶è·ç¦»å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´ï¼‰
        # draw_box_3d ä¾èµ–è¿™ä¸ªå½’ä¸€åŒ–å€¼æ¥è®¡ç®—ç«‹ä½“æ„Ÿ (0=è¿‘, 1=è¿œ)
        self.DEPTH_MIN = 0.5  # æœ€å°å¯ä¿¡è·ç¦»ï¼ˆç±³ï¼‰
        self.DEPTH_MAX = 8.0  # æœ€å¤§æœ‰æ•ˆè·ç¦»ï¼ˆç±³ï¼‰

        # FPS monitoring attributes
        self.camera_fps: Dict[int, float] = {}
        self.camera_frame_times: Dict[int, List[float]] = {}
        self.fps_containers: Dict[int, Any] = {}
        self.overall_fps_container = None

        # Detection result saving attributes
        self.save_detections = False
        self.save_images = True
        self.save_labels = True
        self.save_directory = "./detection_results"
        self.detection_count: Dict[int, int] = {}

        # Detection Statistics
        self.class_detection_stats: Dict[int, Dict[str, int]] = {}
        self.last_detection_time: Dict[int, float] = {}

        # Display options
        self.show_class_names = True

        self.temp_dict = {"model": None, **kwargs}
        self.model_path = None
        if self.temp_dict["model"] is not None:
            self.model_path = self.temp_dict["model"]

        LOGGER.info(f"Ultralytics Solutions: âœ… {self.temp_dict}")

    # ã€æ–°å¢ã€‘è·ç¦»å½’ä¸€åŒ–å‡½æ•°
    def _normalize_distance_to_depth_value(self, distance: float) -> float:
        """
        å°†ç±³åˆ¶è·ç¦» (dist) å½’ä¸€åŒ–åˆ° 0.0 - 1.0 çš„èŒƒå›´ï¼Œç”¨äº draw_box_3d çš„ depth_value è¾“å…¥ã€‚
        (0.0 ä»£è¡¨è¿‘ï¼Œ1.0 ä»£è¡¨è¿œ)
        """
        if distance == float('inf'):
            return 1.0
        
        # æˆªæ–­èŒƒå›´
        dist = np.clip(distance, self.DEPTH_MIN, self.DEPTH_MAX)
        
        # å½’ä¸€åŒ–: (dist - MIN) / (MAX - MIN)
        normalized_depth = (dist - self.DEPTH_MIN) / (self.DEPTH_MAX - self.DEPTH_MIN)
        
        # ç¡®ä¿è¾“å‡ºåœ¨ 0 åˆ° 1 ä¹‹é—´
        return float(np.clip(normalized_depth, 0.0, 1.0))
    
    # æ ¸å¿ƒæµ‹è·ä¸æµ‹è§’å‡½æ•°
    def calculate_spatial_info(self, bbox_xyxy, class_id, cam_index=0):
        """
        è®¡ç®—ç›®æ ‡çš„è·ç¦»å’Œæ–¹ä½è§’
        :param bbox_xyxy: [x1, y1, x2, y2]
        :param class_id: ç±»åˆ«ID
        :param cam_index: æ‘„åƒå¤´ç´¢å¼• (ç”¨äºè®¡ç®—å…¨å±€è§’åº¦)
        :return: distance (m), azimuth (deg, ç›¸å¯¹ç›¸æœº), global_angle (deg, æœºä½“ç³»)
        """
        x1, y1, x2, y2 = bbox_xyxy

        # 1. è®¡ç®—åƒç´ å‚æ•°
        box_width_px = x2 - x1
        x_center_px = (x1 + x2) / 2

        if box_width_px <= 0:
            return float('inf'), 0.0, 0.0

        # 2. è·å–çœŸå®ç‰©ç†å®½åº¦
        real_width = self.class_real_widths.get(class_id, self.default_real_width)

        # 3. è·ç¦»è®¡ç®— (ç›¸ä¼¼ä¸‰è§’å½¢: Z = (W_real * f) / W_pixel)
        fx = self.camera_intrinsics['fx']
        distance = (real_width * fx) / box_width_px

        # 4. æ–¹ä½è§’è®¡ç®— (Atanæ¨¡å‹)
        cx = self.camera_intrinsics['cx']
        pixel_offset = x_center_px - cx
        angle_rad = math.atan(pixel_offset / fx)
        azimuth_deg = math.degrees(angle_rad)

        # 5. å…¨å±€è§’åº¦ (åŠ ä¸Šç›¸æœºå®‰è£…è§’åº¦)
        cam_offset = self.camera_yaw_offsets.get(cam_index, 0.0)
        global_angle = (azimuth_deg + cam_offset + 360) % 360

        return distance, azimuth_deg, global_angle

    def web_ui(self) -> None:
        """Set up the Streamlit web interface."""
        menu_style_cfg = """<style>MainMenu {visibility: hidden;}</style>"""

        main_title_cfg = """<div><h1 style="color:#111F68; text-align:center; font-size:40px; margin-top:-50px;
        font-family: 'Archivo', sans-serif; margin-bottom:20px;">Multi-Camera Spatial Detection</h1></div>"""

        sub_title_cfg = """<div><h5 style="color:#042AFF; text-align:center; font-family: 'Archivo', sans-serif;
        margin-top:-15px; margin-bottom:50px;">Real-time detection & Pseudo 3D Distance Estimation ğŸš—ğŸ“¹ğŸ“</h5></div>"""

        self.st.set_page_config(page_title="Ultralytics Streamlit App", layout="wide")
        self.st.markdown(menu_style_cfg, unsafe_allow_html=True)
        self.st.markdown(main_title_cfg, unsafe_allow_html=True)
        self.st.markdown(sub_title_cfg, unsafe_allow_html=True)

    def sidebar(self) -> None:
        """Configure the Streamlit sidebar settings."""
        with self.st.sidebar:
            logo = "https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Original.svg"
            self.st.image(logo, width=250)

        self.st.sidebar.title("User Configuration")
        self.st.sidebar.markdown("### ğŸ¥ Source: **Multi-Webcam**")

        # Camera configuration
        self.st.sidebar.subheader("Camera Settings")
        self.st.sidebar.info("ğŸ’¡ **Multi-Camera Setup**: Optimized for 4 cameras.")
        self.num_cameras = self.st.sidebar.slider("Number of Cameras", 1, 8, 4)

        if self.num_cameras <= 4:
            available_cameras = [0, 2, 4, 6]
            self.camera_indices = available_cameras[:self.num_cameras]
        else:
            preset_configs = {
                "Sequential (0,1,2,3,4,5,6,7)": ",".join(map(str, range(self.num_cameras))),
                "Reliable (0,2,4,6)": "0,2,4,6",
                "Custom": "custom"
            }
            selected_preset = self.st.sidebar.selectbox("Camera Configuration", list(preset_configs.keys()))

            if selected_preset == "Custom":
                camera_indices_str = self.st.sidebar.text_input(
                    "Camera Indices (comma-separated)",
                    ",".join(map(str, self.camera_indices[:self.num_cameras]))
                )
            else:
                camera_indices_str = preset_configs[selected_preset]

            try:
                self.camera_indices = [int(x.strip()) for x in camera_indices_str.split(",")][:self.num_cameras]
                self.num_cameras = len(self.camera_indices)
            except ValueError:
                self.st.sidebar.error("Invalid camera indices format. Using default values.")
                self.camera_indices = [0, 2, 4, 6]
                self.num_cameras = 4

        # Resolution settings
        self.frame_width = 640
        self.frame_height = 480
        self.st.sidebar.info(f"ğŸ¯ **Resolution**: {self.frame_width}x{self.frame_height}")

        self.enable_trk = self.st.sidebar.radio("Enable Tracking", ("Yes", "No")) == "Yes"

        self.conf = float(self.st.sidebar.slider("Confidence Threshold", 0.0, 1.0, self.conf, 0.01))
        self.iou = float(self.st.sidebar.slider("IoU Threshold", 0.0, 1.0, self.iou, 0.01))

        # Display options
        self.st.sidebar.subheader("Display Options")
        self.show_class_names = self.st.sidebar.checkbox(
            "Show Class Names",
            value=self.show_class_names,
            help="If unchecked, only class IDs will be shown instead of class names"
        )
        # [æ–°å¢] æ˜¾ç¤ºè·ç¦»å¼€å…³
        self.show_distance = self.st.sidebar.checkbox(
            "Show Distance & Pseudo 3D Box",
            value=True,
            help="Show calculated distance (m) and angle (deg) using a pseudo 3D box"
        )

        # Detection saving configuration
        self.st.sidebar.subheader("Detection Saving")
        self.save_detections = self.st.sidebar.checkbox("Save Detection Results", value=self.save_detections)

        if self.save_detections:
            self.save_directory = self.st.sidebar.text_input("Save Directory", value=self.save_directory)
            self.save_images = self.st.sidebar.checkbox("Save Images", value=self.save_images)
            self.save_labels = self.st.sidebar.checkbox("Save Labels (YOLO format)", value=self.save_labels)

            if self.save_directory:
                Path(self.save_directory).mkdir(parents=True, exist_ok=True)
                self.st.sidebar.success(f"Save directory: {self.save_directory}")

        # Create UI layout
        self.overall_fps_container = self.st.empty()
        cols_per_row = 2 if self.num_cameras <= 4 else 3
        rows = (self.num_cameras + cols_per_row - 1) // cols_per_row

        for row in range(rows):
            cols = self.st.columns(cols_per_row)
            for col_idx in range(cols_per_row):
                cam_idx = row * cols_per_row + col_idx
                if cam_idx < self.num_cameras:
                    with cols[col_idx]:
                        self.st.markdown(f"**Camera {self.camera_indices[cam_idx]}**")
                        fps_container = self.st.empty()
                        org_container = self.st.empty()
                        ann_container = self.st.empty()
                        self.camera_containers[cam_idx] = (org_container, ann_container)
                        self.fps_containers[cam_idx] = fps_container

    def source_upload(self) -> None:
        """Handle video file uploads."""
        # ä»…ä¿ç•™å¤šæ‘„åƒå¤´é€»è¾‘ï¼Œæ¸…ç©ºæ­¤å‡½æ•°
        pass

    def configure(self) -> None:
        """Configure the model and load selected classes."""
        available_models = ["Custom Car Detection Model (best.engine)"]
        custom_model_path = "/home/nvidia/Downloads/Ros/ballCar2/weights/weights/best.engine"

        if self.model_path:
            available_models.insert(0, self.model_path)

        selected_model = self.st.sidebar.selectbox("Model", available_models, index=0)

        with self.st.spinner("Model is loading..."):
            if selected_model == "Custom Car Detection Model (best.engine)":
                model_path = custom_model_path
            elif selected_model.endswith((".pt", ".onnx", ".torchscript", ".mlpackage", ".engine")):
                model_path = selected_model
            else:
                model_path = custom_model_path

            self.model = YOLO(model_path)
            class_names = list(self.model.names.values())
        self.st.success("Model loaded successfully!")

        selected_classes = self.st.sidebar.multiselect("Classes", class_names, default=class_names)
        self.selected_ind = [class_names.index(option) for option in selected_classes]
        if not isinstance(self.selected_ind, list):
            self.selected_ind = list(self.selected_ind)

    def setup_cameras(self) -> bool:
        """Initialize multiple cameras."""
        self.camera_caps = []
        self.camera_frames = {}
        self.camera_fps = {}
        self.camera_frame_times = {}
        successful_cameras = 0

        for i, cam_idx in enumerate(self.camera_indices):
            try:
                cap = cv2.VideoCapture(cam_idx)
                if cap.isOpened():
                    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
                    cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.frame_width)
                    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.frame_height)
                    time.sleep(0.1)

                    test_success = False
                    for _ in range(3):
                        ret, test_frame = cap.read()
                        if ret and test_frame is not None and test_frame.size > 0:
                            test_success = True
                            break
                        time.sleep(0.05)

                    if test_success:
                        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        self.camera_caps.append(cap)
                        self.camera_frames[i] = np.zeros((actual_height, actual_width, 3), dtype=np.uint8)
                        self.camera_fps[i] = 0.0
                        self.camera_frame_times[i] = []
                        successful_cameras += 1
                        LOGGER.info(f"Camera {cam_idx} initialized successfully: {actual_width}x{actual_height}")
                    else:
                        self.camera_caps.append(None)
                        cap.release()
                        LOGGER.warning(f"Camera {cam_idx} opened but cannot read frames")
                else:
                    self.camera_caps.append(None)
                    LOGGER.warning(f"Failed to open camera {cam_idx}")
                time.sleep(0.2)
            except Exception as e:
                self.camera_caps.append(None)
                LOGGER.error(f"Error initializing camera {cam_idx}: {e}")

        return successful_cameras > 0

    def camera_thread(self, camera_index: int, cap: cv2.VideoCapture) -> None:
        """Thread function to capture frames from a single camera."""
        consecutive_failures = 0
        max_failures = 10

        while self.camera_running and cap is not None and cap.isOpened():
            try:
                ret, frame = cap.read()
                if ret and frame is not None and frame.size > 0:
                    self.camera_frames[camera_index] = frame.copy()

                    current_time = time.time()
                    if camera_index in self.camera_frame_times:
                        self.camera_frame_times[camera_index].append(current_time)
                        if len(self.camera_frame_times[camera_index]) > 30:
                            self.camera_frame_times[camera_index].pop(0)

                        if len(self.camera_frame_times[camera_index]) > 1:
                            time_diff = self.camera_frame_times[camera_index][-1] - self.camera_frame_times[camera_index][0]
                            if time_diff > 0:
                                self.camera_fps[camera_index] = (len(self.camera_frame_times[camera_index]) - 1) / time_diff

                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    if consecutive_failures >= max_failures:
                        LOGGER.error(f"Camera {camera_index} failed max times, stopping thread")
                        break
                time.sleep(0.01)
            except Exception as e:
                consecutive_failures += 1
                LOGGER.error(f"Error in camera thread {camera_index}: {e}")
                if consecutive_failures >= max_failures:
                    break
                time.sleep(0.1)

    def multi_camera_inference(self) -> None:
        """Perform inference on multiple cameras."""
        self.st.info(f"ğŸ¥ Attempting to initialize {len(self.camera_indices)} cameras: {self.camera_indices}")

        if not self.setup_cameras():
            self.st.error("âŒ Failed to initialize any cameras.")
            return

        working_cameras = sum(1 for cap in self.camera_caps if cap is not None)
        self.st.success(f"âœ… {working_cameras} cameras initialized successfully!")

        self.camera_running = True
        self.camera_threads = []

        for i, cap in enumerate(self.camera_caps):
            if cap is not None:
                thread = threading.Thread(target=self.camera_thread, args=(i, cap))
                thread.daemon = True
                thread.start()
                self.camera_threads.append(thread)

        stop_button = self.st.sidebar.button("Stop")

        try:
            inference_start_time = time.time()
            total_frames_processed = 0

            while self.camera_running:
                if stop_button:
                    break
                
                # ç”¨äºæ¸…ç† tracker çš„ ID åˆ—è¡¨
                active_ids = []

                for cam_idx in range(len(self.camera_caps)):
                    if cam_idx in self.camera_frames and cam_idx in self.camera_containers:
                        frame = self.camera_frames[cam_idx].copy()
                        real_cam_id = self.camera_indices[cam_idx]  # çœŸå®çš„USB ID (0,2,4,6)

                        if frame is not None and frame.size > 0:
                            if self.enable_trk:
                                results = self.model.track(
                                    frame, conf=self.conf, iou=self.iou, classes=self.selected_ind,
                                    persist=True, imgsz=(480, 640)
                                )
                            else:
                                results = self.model(frame, conf=self.conf, iou=self.iou, classes=self.selected_ind,
                                                     imgsz=(480, 640))

                            # YOLOè‡ªå¸¦çš„2Dæ ‡æ³¨ (åé¢ä¼šè¢«3Dæ¡†è¦†ç›–)
                            annotated_frame = results[0].plot(labels=self.show_class_names)

                            # --- START: ç»˜åˆ¶ Pseudo 3D æ¡†å’Œè·ç¦»ä¿¡æ¯ ---
                            if self.show_distance and results[0].boxes is not None:
                                for box in results[0].boxes:
                                    # 1. è·å– Box ä¿¡æ¯
                                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                                    class_id = int(box.cls.item())
                                    # score = box.conf.item() # å·²æ³¨é‡Šï¼Œä¸ä¼ å…¥score
                                    class_name = self.model.names[class_id]
                                    obj_id = box.id.item() if box.id is not None else None
                                    
                                    if obj_id is not None:
                                        active_ids.append(obj_id)

                                    # 2. è®¡ç®—è·ç¦»ã€æ–¹ä½è§’
                                    dist, azim, global_ang = self.calculate_spatial_info(
                                        (x1, y1, x2, y2),
                                        class_id,
                                        real_cam_id
                                    )

                                    # 3. å°†ç±³åˆ¶è·ç¦»è½¬æ¢ä¸ºå½’ä¸€åŒ–æ·±åº¦å€¼ (0.0-1.0)
                                    depth_value_norm = self._normalize_distance_to_depth_value(dist)
                                    
                                    # 4. æ„é€  draw_box_3d æ‰€éœ€çš„ box_3d å­—å…¸
                                    # ã€ä¿®æ”¹ã€‘ç§»é™¤äº† 'score' å­—æ®µï¼Œé˜²æ­¢æ˜¾ç¤ºåˆ†æ•°
                                    box_3d_input = {
                                        'bbox_2d': (x1, y1, x2, y2),
                                        'depth_value': depth_value_norm, # ç”¨äºè®¡ç®—åç§»é‡å’Œç«‹ä½“æ„Ÿ
                                        # 'depth_method': f"Mono({dist:.2f}m, {azim:.1f}Â°)", # ç”¨äºæ˜¾ç¤ºè·ç¦»
                                        'depth_method': f"Mono{dist:.2f}m", # ç”¨äºæ˜¾ç¤ºè·ç¦»
                                        'class_name': class_name,
                                        'object_id': obj_id
                                        # 'score': score 
                                    }
                                    
                                    # 5. ç»˜åˆ¶ä¼ª 3D æ¡† (draw_box_3d ä¼šç›´æ¥ä¿®æ”¹ annotated_frame)
                                    # æ ¹æ®ç±»åˆ«é€‰æ‹©é¢œè‰²
                                    if 'car' in class_name.lower():
                                        color = (0, 255, 0)  # çº¢è‰²
                                    elif 'person' in class_name.lower():
                                        color = (0, 255, 0)  # ç»¿è‰²
                                    else:
                                        color = (0, 0, 255) # ç™½è‰²
                                        
                                    annotated_frame = self.bbox3d_estimator.draw_box_3d(
                                        annotated_frame, box_3d_input, color=color
                                    )
                                    
                            # --- END: ç»˜åˆ¶ Pseudo 3D æ¡†å’Œè·ç¦»ä¿¡æ¯ ---

                            # Save detection results if enabled
                            self.save_detection_results(results, frame, real_cam_id)

                            # Update UI
                            org_container, ann_container = self.camera_containers[cam_idx]
                            org_container.image(frame, channels="BGR", caption=f"Camera {real_cam_id} - Original")
                            ann_container.image(annotated_frame, channels="BGR",
                                                caption=f"Camera {real_cam_id} - Predicted")

                            if cam_idx in self.fps_containers:
                                self.fps_containers[cam_idx].metric(
                                    f"Camera {real_cam_id} FPS",
                                    f"{self.camera_fps.get(cam_idx, 0.0):.1f}"
                                )

                            total_frames_processed += 1
                
                # å¦‚æœå¼€å¯è¿½è¸ªï¼Œæ¸…ç†ä¸æ´»è·ƒçš„IDç¼“å­˜
                if self.enable_trk:
                    self.bbox3d_estimator.cleanup_trackers(active_ids)

                if self.overall_fps_container:
                    elapsed_time = time.time() - inference_start_time
                    if elapsed_time > 0:
                        overall_fps = total_frames_processed / elapsed_time
                        avg_camera_fps = sum(self.camera_fps.values()) / len(
                            self.camera_fps) if self.camera_fps else 0

                        self.overall_fps_container.markdown(f"""
                        ### ğŸ“Š Performance Metrics
                        - **Overall Processing FPS**: {overall_fps:.1f}
                        - **Average Camera FPS**: {avg_camera_fps:.1f}
                        - **Active Cameras**: {working_cameras}
                        """)

                time.sleep(0.01)

        finally:
            self.camera_running = False
            for thread in self.camera_threads:
                if thread.is_alive():
                    thread.join(timeout=1.0)
            for cap in self.camera_caps:
                if cap is not None:
                    cap.release()

    def save_detection_results(self, results, frame: np.ndarray, camera_id: int) -> None:
        """Save detection results to specified directory."""
        if not self.save_detections or not results[0].boxes:
            return

        # Console logging for detections
        detected_classes = []
        if results[0].boxes is not None:
            for box in results[0].boxes:
                class_id = int(box.cls.item())
                class_name = self.model.names[class_id]
                detected_classes.append(class_name)

        current_time = time.time()

        # Stats tracking
        if camera_id not in self.class_detection_stats:
            self.class_detection_stats[camera_id] = {}

        for class_name in detected_classes:
            if class_name not in self.class_detection_stats[camera_id]:
                self.class_detection_stats[camera_id][class_name] = 0
            self.class_detection_stats[camera_id][class_name] += 1

        self.last_detection_time[camera_id] = current_time

        # Directory setup
        if camera_id not in self.detection_count:
            self.detection_count[camera_id] = 0

        camera_dir = Path(self.save_directory) / f"camera_{camera_id}"
        camera_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        self.detection_count[camera_id] += 1
        base_filename = f"detection_{timestamp}_{self.detection_count[camera_id]:04d}"

        if self.save_images:
            img_path = camera_dir / f"{base_filename}.jpg"
            # ä¸ºäº†ä¿å­˜ 3D æ•ˆæœï¼Œéœ€è¦åŸºäºåŸå§‹å¸§é‡æ–°ç»˜åˆ¶
            annotated_frame_save = frame.copy()
            
            if results[0].boxes is not None:
                for box in results[0].boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    class_id = int(box.cls.item())
                    # score = box.conf.item() # å·²æ³¨é‡Š
                    class_name = self.model.names[class_id]
                    obj_id = box.id.item() if box.id is not None else None
                    
                    dist, azim, global_ang = self.calculate_spatial_info(
                        (x1, y1, x2, y2), class_id, camera_id
                    )
                    depth_value_norm = self._normalize_distance_to_depth_value(dist)

                    box_3d_input = {
                        'bbox_2d': (x1, y1, x2, y2),
                        'depth_value': depth_value_norm,
                        'depth_method': f"Mono({dist:.2f}m)",
                        'class_name': class_name,
                        'object_id': obj_id
                        # 'score': score # å·²ç§»é™¤
                    }

                    if 'car' in class_name.lower():
                        color = (0, 0, 255)  
                    elif 'person' in class_name.lower():
                        color = (0, 255, 0)
                    else:
                        color = (255, 255, 255)
                        
                    annotated_frame_save = self.bbox3d_estimator.draw_box_3d(
                        annotated_frame_save, box_3d_input, color=color
                    )

            cv2.imwrite(str(img_path), annotated_frame_save)

        if self.save_labels:
            label_path = camera_dir / f"{base_filename}.txt"
            self._save_yolo_labels(results[0], label_path, frame.shape, camera_id)

    def _save_yolo_labels(self, result, label_path: Path, img_shape: tuple, cam_id=0) -> None:
        """Save detection results in YOLO format + Distance Info."""
        if not result.boxes:
            return

        height, width = img_shape[:2]

        with open(label_path, 'w') as f:
            for box in result.boxes:
                class_id = int(box.cls.item())
                confidence = float(box.conf.item())
                x1, y1, x2, y2 = box.xyxy[0].tolist()

                # YOLO Standard Format
                x_center = (x1 + x2) / 2 / width
                y_center = (y1 + y2) / 2 / height
                bbox_width = (x2 - x1) / width
                bbox_height = (y2 - y1) / height

                # è®¡ç®—è·ç¦»å¹¶åœ¨Logä¸­ä¿å­˜
                dist, azim, global_ang = self.calculate_spatial_info((x1, y1, x2, y2), class_id, cam_id)

                # æ‰©å±•ä¿å­˜æ ¼å¼: class x y w h conf dist azimuth global_angle
                f.write(
                    f"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f} {confidence:.6f} {dist:.3f} {azim:.1f} {global_ang:.1f}\n")

    def inference(self) -> None:
        """Perform real-time object detection inference."""
        self.web_ui()
        self.sidebar()
        self.source_upload()
        self.configure()

        if self.st.sidebar.button("Start"):
            # åªæœ‰å¤šæ‘„åƒå¤´æ¨¡å¼
            self.multi_camera_inference()
        else:
            self.st.info("Click 'Start' in the sidebar to begin Multi-Camera Detection.")


if __name__ == "__main__":
    args = len(sys.argv)
    model = sys.argv[1] if args > 1 else None
    Inference(model=model).inference()
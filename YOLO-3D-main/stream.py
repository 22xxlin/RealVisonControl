import sklearn
from sklearn import metrics # [é‡è¦] å¿…é¡»æ”¾åœ¨ç¬¬ä¸€è¡Œï¼Œä¿®å¤ TLS æŠ¥é”™
import streamlit as st
import cv2
import numpy as np
import torch
import time
import sys
import os

# å¿½ç•¥ Apple Silicon è®¾ç½®
if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

# å¯¼å…¥ä½ çš„æ¨¡å—
from detection_model import ObjectDetector
from depth_model import DepthEstimator
from bbox3d_utils import BBox3DEstimator, BirdEyeView

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="VSWARM 3D Perception", layout="wide")
st.title("ğŸ¤– VSWARM: 3D Object Detection & Depth")
st.markdown("YOLOv11 + Depth Anything V2 | Running on Jetson")

# --- ä¾§è¾¹æ è®¾ç½® ---
st.sidebar.header("ç³»ç»Ÿè®¾ç½®")
camera_index = st.sidebar.selectbox("æ‘„åƒå¤´ ID", [0, 1, 2, 3], index=0)
conf_threshold = st.sidebar.slider("æ£€æµ‹é˜ˆå€¼ (Confidence)", 0.1, 1.0, 0.40)
enable_bev = st.sidebar.checkbox("æ˜¾ç¤ºä¿¯è§†å›¾ (BEV)", value=True)
enable_depth_overlay = st.sidebar.checkbox("æ˜¾ç¤ºæ·±åº¦å›¾ (ç”»ä¸­ç”»)", value=True)

# --- æ¨¡å‹åŠ è½½ (ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½) ---
@st.cache_resource
def load_models():
    print("ğŸš€ Loading AI Models...")
    device = 'cpu' # å¼ºåˆ¶ CPU ä¿è¯ç¨³å®šæ€§
    
    # åŠ è½½æ£€æµ‹å™¨
    try:
        detector = ObjectDetector(model_size="nano", conf_thres=0.25, device=device)
    except Exception as e:
        st.error(f"Detector Load Error: {e}")
        detector = ObjectDetector(model_size="nano", conf_thres=0.25, device='cpu')
    
    # åŠ è½½æ·±åº¦ä¼°è®¡å™¨
    try:
        depth_estimator = DepthEstimator(model_size="small", device=device)
    except Exception as e:
        st.error(f"Depth Load Error: {e}")
        depth_estimator = DepthEstimator(model_size="small", device='cpu')
        
    bbox3d_estimator = BBox3DEstimator()
    bev = BirdEyeView(scale=60, size=(300, 300))
    
    print("âœ… Models Loaded!")
    return detector, depth_estimator, bbox3d_estimator, bev

# åŠ è½½æ¨¡å‹
with st.spinner('æ­£åœ¨åˆå§‹åŒ– AI å¼•æ“ (é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹)...'):
    detector, depth_estimator, bbox3d_estimator, bev = load_models()
    # åŠ¨æ€æ›´æ–°é˜ˆå€¼
    detector.conf_thres = conf_threshold

# --- ä¸»ç•Œé¢å¸ƒå±€ ---
col1, col2 = st.columns([3, 1])
with col1:
    st_frame = st.empty() # è§†é¢‘ç”»é¢å®¹å™¨
with col2:
    st_fps = st.empty()   # FPS æ˜¾ç¤ºå®¹å™¨
    st_info = st.empty()  # æ£€æµ‹ä¿¡æ¯å®¹å™¨
    start_btn = st.button("â–¶ï¸ å¼€å§‹æ¨æµ (Start)", type="primary")
    stop_btn = st.button("â¹ï¸ åœæ­¢æ¨æµ (Stop)")

def main():
    if start_btn:
        # å°è¯•æ‰“å¼€æ‘„åƒå¤´ (ä¼˜å…ˆä½¿ç”¨ V4L2 åç«¯)
        cap = cv2.VideoCapture(camera_index, cv2.CAP_V4L2)
        if not cap.isOpened():
            cap = cv2.VideoCapture(camera_index)
            
        if not cap.isOpened():
            st.error(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}ã€‚è¯·æ£€æŸ¥è¿æ¥æˆ–å°è¯•å…¶ä»– IDã€‚")
            return

        # é™ä½åˆ†è¾¨ç‡ä»¥æå‡æµä¼ è¾“é€Ÿåº¦
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        frame_count = 0
        start_time = time.time()

        try:
            while cap.isOpened():
                if stop_btn:
                    break

                ret, frame = cap.read()
                if not ret:
                    st.warning("æ— æ³•è¯»å–è§†é¢‘å¸§")
                    break
                
                # å¤åˆ¶å¸§ç”¨äºä¸åŒå¤„ç†
                original_frame = frame.copy()
                detection_frame = frame.copy()
                result_frame = frame.copy()
                
                # --- 1. ç›®æ ‡æ£€æµ‹ ---
                detection_frame, detections = detector.detect(detection_frame, track=True)
                
                # --- 2. æ·±åº¦ä¼°è®¡ ---
                depth_map = depth_estimator.estimate_depth(original_frame)
                
                # --- 3. 3D è®¡ç®— ---
                boxes_3d = []
                active_ids = []
                
                for detection in detections:
                    bbox, score, class_id, obj_id = detection
                    class_name = detector.get_class_names()[class_id]
                    
                    # æ·±åº¦é‡‡æ ·ç­–ç•¥
                    if class_name.lower() in ['person', 'cat', 'dog']:
                        center_x = int((bbox[0] + bbox[2]) / 2)
                        center_y = int((bbox[1] + bbox[3]) / 2)
                        depth_value = depth_estimator.get_depth_at_point(depth_map, center_x, center_y)
                        depth_method = 'center'
                    else:
                        depth_value = depth_estimator.get_depth_in_region(depth_map, bbox, method='median')
                        depth_method = 'median'
                    
                    box_3d = {
                        'bbox_2d': bbox, 'depth_value': depth_value, 'depth_method': depth_method,
                        'class_name': class_name, 'object_id': obj_id, 'score': score
                    }
                    boxes_3d.append(box_3d)
                    if obj_id is not None: active_ids.append(obj_id)
                
                # æ¸…ç†è¿½è¸ªå™¨
                bbox3d_estimator.cleanup_trackers(active_ids)
                
                # --- 4. ç»˜åˆ¶ 3D æ¡† ---
                for box_3d in boxes_3d:
                    class_name = box_3d['class_name'].lower()
                    color = (0, 255, 0) if 'person' in class_name else (255, 100, 0) # äººæ˜¯ç»¿è‰²ï¼Œå…¶ä»–æ˜¯è“è‰²
                    result_frame = bbox3d_estimator.draw_box_3d(result_frame, box_3d, color=color)
                
                # --- 5. å åŠ ä¿¯è§†å›¾ (BEV) ---
                if enable_bev:
                    bev.reset()
                    for box_3d in boxes_3d: bev.draw_box(box_3d)
                    bev_image = bev.get_image()
                    
                    # è°ƒæ•´å¤§å°æ”¾åœ¨å·¦ä¸‹è§’
                    bev_h = height // 3
                    bev_w = bev_h
                    bev_resized = cv2.resize(bev_image, (bev_w, bev_h))
                    
                    # ç®€å•çš„é®ç½©å åŠ 
                    result_frame[height - bev_h:height, 0:bev_w] = bev_resized
                    cv2.rectangle(result_frame, (0, height - bev_h), (bev_w, height), (255, 255, 255), 2)
                    cv2.putText(result_frame, "BEV Map", (5, height - bev_h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)

                # --- 6. å åŠ æ·±åº¦å›¾ (ç”»ä¸­ç”») ---
                if enable_depth_overlay:
                    depth_colored = depth_estimator.colorize_depth(depth_map)
                    d_h = height // 4
                    d_w = int(d_h * (width / height))
                    d_resized = cv2.resize(depth_colored, (d_w, d_h))
                    # æ”¾åœ¨å·¦ä¸Šè§’
                    result_frame[0:d_h, 0:d_w] = d_resized
                    cv2.rectangle(result_frame, (0, 0), (d_w, d_h), (255, 255, 255), 1)

                # --- è®¡ç®— FPS ---
                frame_count += 1
                elapsed = time.time() - start_time
                fps = frame_count / elapsed if elapsed > 0 else 0
                
                # BGR è½¬ RGB ä¾›ç½‘é¡µæ˜¾ç¤º
                frame_rgb = cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB)
                
                # --- æ›´æ–°ç½‘é¡µ ---
                st_frame.image(frame_rgb, channels="RGB", use_container_width=True)
                
                st_fps.metric("å®æ—¶å¸§ç‡ (FPS)", f"{fps:.1f}")
                
                # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„ç‰©ä½“åˆ—è¡¨
                if boxes_3d:
                    info_text = "**æ£€æµ‹ç‰©ä½“:**\n"
                    for b in boxes_3d:
                        dist = b['depth_value']
                        name = b['class_name']
                        info_text += f"- {name}: {dist:.2f}m\n"
                    st_info.markdown(info_text)
                else:
                    st_info.markdown("*æš‚æ— ç›®æ ‡*")

        except Exception as e:
            st.error(f"è¿è¡Œå‡ºé”™: {e}")
        finally:
            cap.release()
            st.info("æ‘„åƒå¤´å·²é‡Šæ”¾")

if __name__ == "__main__":
    main()